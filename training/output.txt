--output_dir ./flamingo-coco --run_name flamingo-tiny-vitL --do_train --do_eval --optim adamw_torch --learning_rate 0.0001 --warmup_steps 5000 --lr_scheduler_type constant_with_warmup --per_device_train_batch_size 8 --per_device_eval_batch_size 64 --gradient_accumulation_steps 1 --evaluation_strategy steps --eval_steps 1000 --save_strategy epoch --save_total_limit 2 --log_level info --dataloader_num_workers 8 --dataloader_pin_memory True --fp16 --report_to wandb --ddp_find_unused_parameters False
running on a single GPU
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
model info: 
FlamingoOPT(
  (vision_encoder): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(257, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
  (resampler): PerceiverResampler(
    (layers): ModuleList(
      (0-5): 6 x ModuleList(
        (0): PerceiverAttentionLayer(
          (norm_media): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (norm_latents): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=1024, out_features=512, bias=False)
          (to_k): Linear(in_features=1024, out_features=512, bias=False)
          (to_v): Linear(in_features=1024, out_features=512, bias=False)
          (to_out): Linear(in_features=512, out_features=1024, bias=False)
        )
        (1): Sequential(
          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=1024, out_features=4096, bias=False)
          (2): SquaredReLU()
          (3): Linear(in_features=4096, out_features=1024, bias=False)
        )
      )
    )
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (lm): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50273, 768)
      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)
      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0-11): 12 x ModifiedLMBlock(
          (xattn_block): GatedCrossAttentionBlock(
            (attn): MaskedCrossAttention(
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=768, out_features=512, bias=False)
              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)
              (to_out): Linear(in_features=512, out_features=768, bias=False)
            )
            (ffw): Sequential(
              (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=768, out_features=3072, bias=False)
              (2): SquaredReLU()
              (3): Linear(in_features=3072, out_features=768, bias=False)
            )
          )
          (lm_block): OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
  (lm_head): Linear(in_features=768, out_features=50273, bias=False)
)
loading annotations into memory...
Done (t=1.05s)
creating index...
index created!
loading annotations into memory...
Done (t=0.06s)
creating index...
index created!
