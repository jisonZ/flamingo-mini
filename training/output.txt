FlamingoModel(
  (flamingo): FlamingoFLAN(
    (vision_encoder): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(257, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (resampler): PerceiverResampler(
      (layers): ModuleList(
        (0-5): 6 x ModuleList(
          (0): PerceiverAttentionLayer(
            (norm_media): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_latents): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (to_q): Linear(in_features=1024, out_features=512, bias=False)
            (to_k): Linear(in_features=1024, out_features=512, bias=False)
            (to_v): Linear(in_features=1024, out_features=512, bias=False)
            (to_out): Linear(in_features=512, out_features=1024, bias=False)
          )
          (1): Sequential(
            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=1024, out_features=4096, bias=False)
            (2): SquaredReLU()
            (3): Linear(in_features=4096, out_features=1024, bias=False)
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (lm): T5ForConditionalGeneration(
      (shared): Embedding(32129, 1024)
      (encoder): T5Stack(
        (embed_tokens): Embedding(32129, 1024)
        (block): ModuleList(
          (0): ModifiedLMBlock(
            (xattn_block): GatedCrossAttentionBlock(
              (attn): MaskedCrossAttention(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (to_q): Linear(in_features=1024, out_features=512, bias=False)
                (to_kv): Linear(in_features=1024, out_features=1024, bias=False)
                (to_out): Linear(in_features=512, out_features=1024, bias=False)
              )
              (ffw): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=1024, out_features=4096, bias=False)
                (2): SquaredReLU()
                (3): Linear(in_features=4096, out_features=1024, bias=False)
              )
            )
            (lm_block): T5Block(
              (layer): ModuleList(
                (0): T5LayerSelfAttention(
                  (SelfAttention): T5Attention(
                    (q): Linear(in_features=1024, out_features=1024, bias=False)
                    (k): Linear(in_features=1024, out_features=1024, bias=False)
                    (v): Linear(in_features=1024, out_features=1024, bias=False)
                    (o): Linear(in_features=1024, out_features=1024, bias=False)
                    (relative_attention_bias): Embedding(32, 16)
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (1): T5LayerFF(
                  (DenseReluDense): T5DenseGatedActDense(
                    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                    (wo): Linear(in_features=2816, out_features=1024, bias=False)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (act): NewGELUActivation()
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
          (1-23): 23 x ModifiedLMBlock(
            (xattn_block): GatedCrossAttentionBlock(
              (attn): MaskedCrossAttention(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (to_q): Linear(in_features=1024, out_features=512, bias=False)
                (to_kv): Linear(in_features=1024, out_features=1024, bias=False)
                (to_out): Linear(in_features=512, out_features=1024, bias=False)
              )
              (ffw): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=1024, out_features=4096, bias=False)
                (2): SquaredReLU()
                (3): Linear(in_features=4096, out_features=1024, bias=False)
              )
            )
            (lm_block): T5Block(
              (layer): ModuleList(
                (0): T5LayerSelfAttention(
                  (SelfAttention): T5Attention(
                    (q): Linear(in_features=1024, out_features=1024, bias=False)
                    (k): Linear(in_features=1024, out_features=1024, bias=False)
                    (v): Linear(in_features=1024, out_features=1024, bias=False)
                    (o): Linear(in_features=1024, out_features=1024, bias=False)
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (1): T5LayerFF(
                  (DenseReluDense): T5DenseGatedActDense(
                    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                    (wo): Linear(in_features=2816, out_features=1024, bias=False)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (act): NewGELUActivation()
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (decoder): T5Stack(
        (embed_tokens): Embedding(32129, 1024)
        (block): ModuleList(
          (0): ModifiedLMBlock(
            (xattn_block): GatedCrossAttentionBlock(
              (attn): MaskedCrossAttention(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (to_q): Linear(in_features=1024, out_features=512, bias=False)
                (to_kv): Linear(in_features=1024, out_features=1024, bias=False)
                (to_out): Linear(in_features=512, out_features=1024, bias=False)
              )
              (ffw): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=1024, out_features=4096, bias=False)
                (2): SquaredReLU()
                (3): Linear(in_features=4096, out_features=1024, bias=False)
              )
            )
            (lm_block): T5Block(
              (layer): ModuleList(
                (0): T5LayerSelfAttention(
                  (SelfAttention): T5Attention(
                    (q): Linear(in_features=1024, out_features=1024, bias=False)
                    (k): Linear(in_features=1024, out_features=1024, bias=False)
                    (v): Linear(in_features=1024, out_features=1024, bias=False)
                    (o): Linear(in_features=1024, out_features=1024, bias=False)
                    (relative_attention_bias): Embedding(32, 16)
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (1): T5LayerCrossAttention(
                  (EncDecAttention): T5Attention(
                    (q): Linear(in_features=1024, out_features=1024, bias=False)
                    (k): Linear(in_features=1024, out_features=1024, bias=False)
                    (v): Linear(in_features=1024, out_features=1024, bias=False)
                    (o): Linear(in_features=1024, out_features=1024, bias=False)
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (2): T5LayerFF(
                  (DenseReluDense): T5DenseGatedActDense(
                    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                    (wo): Linear(in_features=2816, out_features=1024, bias=False)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (act): NewGELUActivation()
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
          (1-23): 23 x ModifiedLMBlock(
            (xattn_block): GatedCrossAttentionBlock(
              (attn): MaskedCrossAttention(
                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (to_q): Linear(in_features=1024, out_features=512, bias=False)
                (to_kv): Linear(in_features=1024, out_features=1024, bias=False)
                (to_out): Linear(in_features=512, out_features=1024, bias=False)
              )
              (ffw): Sequential(
                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=1024, out_features=4096, bias=False)
                (2): SquaredReLU()
                (3): Linear(in_features=4096, out_features=1024, bias=False)
              )
            )
            (lm_block): T5Block(
              (layer): ModuleList(
                (0): T5LayerSelfAttention(
                  (SelfAttention): T5Attention(
                    (q): Linear(in_features=1024, out_features=1024, bias=False)
                    (k): Linear(in_features=1024, out_features=1024, bias=False)
                    (v): Linear(in_features=1024, out_features=1024, bias=False)
                    (o): Linear(in_features=1024, out_features=1024, bias=False)
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (1): T5LayerCrossAttention(
                  (EncDecAttention): T5Attention(
                    (q): Linear(in_features=1024, out_features=1024, bias=False)
                    (k): Linear(in_features=1024, out_features=1024, bias=False)
                    (v): Linear(in_features=1024, out_features=1024, bias=False)
                    (o): Linear(in_features=1024, out_features=1024, bias=False)
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (2): T5LayerFF(
                  (DenseReluDense): T5DenseGatedActDense(
                    (wi_0): Linear(in_features=1024, out_features=2816, bias=False)
                    (wi_1): Linear(in_features=1024, out_features=2816, bias=False)
                    (wo): Linear(in_features=2816, out_features=1024, bias=False)
                    (dropout): Dropout(p=0.1, inplace=False)
                    (act): NewGELUActivation()
                  )
                  (layer_norm): T5LayerNorm()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (lm_head): Linear(in_features=1024, out_features=32129, bias=False)
    )
    (lm_head): Linear(in_features=1024, out_features=32129, bias=False)
  )
)huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

loading annotations into memory...
Done (t=0.92s)
creating index...
index created!
loading annotations into memory...
Done (t=0.07s)
creating index...
index created!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
